{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Dataset\n",
    "Please contact ouwen@gradienthealth.io about access to this demo notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hub\n",
    "import tensorflow as tf\n",
    "from helper import visualize, get_model, to_model_fit, get_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "Get the structure of the dataset schema and load the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = hub.Dataset(\"s3://snark-gradient-raw-data/transform_50_20000_x-test-3/ds3\", token=get_token())\n",
    "ds.schema \n",
    "ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data\n",
    "Access any other sample and visualize first image in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_sequence = ds[\"image\", 0].compute() \n",
    "visualize(image_sequence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset and Filter\n",
    "Take a subset of the dataset and filter based on only on frontal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ds[500:1000] \n",
    "def only_frontal(sample):\n",
    "    viewPosition = sample[\"viewPosition\"].compute(True)\n",
    "    return True if \"PA\" in viewPosition or \"AP\" in viewPosition else False\n",
    "\n",
    "filtered = subset.filter(only_frontal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Alternatively we can send a subset of keys to t that are relevant for training\n",
    "This is faster as otherwise other irrelevant data is fetched too, that can slow things down\n",
    "converts the data into X, y format format for training, then we\n",
    "batch and prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = filtered.to_tensorflow()\n",
    "tds = filtered.to_tensorflow(key_list=[\"image\", \"label_chexpert\", \"viewPosition\"])\n",
    "tds_train = tds.map(to_model_fit)\n",
    "tds_train = tds_train.batch(8).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 512, 512, 8)       224       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 8)       584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 256, 256, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 16)      1168      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 32)      4640      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 131072)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               33554688  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                3598      \n",
      "=================================================================\n",
      "Total params: 33,853,334\n",
      "Trainable params: 33,853,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 14s 94ms/step - loss: 3.5857 - mse: 7.1714 - mae: 2.5643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb69057e128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=['mse', 'mae'],\n",
    "              loss_weights=[0.5, 0.5],\n",
    "              metrics=['mse', 'mae'])\n",
    "\n",
    "model.fit(tds_train,\n",
    "          steps_per_epoch=50,\n",
    "          validation_steps=7,\n",
    "          epochs=1,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
