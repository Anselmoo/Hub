{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "import pickle\n",
    "from configparser import ConfigParser\n",
    "# from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "# from keras.optimizers import Adam\n",
    "# from models.keras import ModelFactory\n",
    "from utility import get_sample_counts\n",
    "import tensorflow as tf\n",
    "import hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_frontal(sample):\n",
    "    viewPosition = sample[\"viewPosition\"].compute(True)\n",
    "    return True if \"PA\" in viewPosition or \"AP\" in viewPosition else False\n",
    "\n",
    "\n",
    "def get_image(viewPosition, images):\n",
    "    for i, vp in enumerate(viewPosition):\n",
    "        if vp in [5, 12]:\n",
    "            return np.concatenate((images[i], images[i], images[i]), axis=2)\n",
    "\n",
    "\n",
    "def to_model_fit(sample):\n",
    "    viewPosition = sample[\"viewPosition\"]\n",
    "    images = sample[\"image\"]\n",
    "    image = tf.py_function(get_image, [viewPosition, images], tf.uint16)\n",
    "    labels = sample[\"label_chexpert\"]\n",
    "    return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser config\n",
    "config_file = \"./config.ini\"\n",
    "cp = ConfigParser()\n",
    "cp.read(config_file)\n",
    "\n",
    "# default config\n",
    "output_dir = cp[\"DEFAULT\"].get(\"output_dir\")\n",
    "base_model_name = cp[\"DEFAULT\"].get(\"base_model_name\")\n",
    "print(base_model_name)\n",
    "class_names = cp[\"DEFAULT\"].get(\"class_names\").split(\",\")\n",
    "\n",
    "# train config\n",
    "use_base_model_weights = cp[\"TRAIN\"].getboolean(\"use_base_model_weights\")\n",
    "use_trained_model_weights = cp[\"TRAIN\"].getboolean(\"use_trained_model_weights\")\n",
    "use_best_weights = cp[\"TRAIN\"].getboolean(\"use_best_weights\")\n",
    "output_weights_name = cp[\"TRAIN\"].get(\"output_weights_name\")\n",
    "epochs = cp[\"TRAIN\"].getint(\"epochs\")\n",
    "batch_size = cp[\"TRAIN\"].getint(\"batch_size\")\n",
    "initial_learning_rate = cp[\"TRAIN\"].getfloat(\"initial_learning_rate\")\n",
    "generator_workers = cp[\"TRAIN\"].getint(\"generator_workers\")\n",
    "image_dimension = cp[\"TRAIN\"].getint(\"image_dimension\")\n",
    "train_steps = cp[\"TRAIN\"].get(\"train_steps\")\n",
    "patience_reduce_lr = cp[\"TRAIN\"].getint(\"patience_reduce_lr\")\n",
    "min_lr = cp[\"TRAIN\"].getfloat(\"min_lr\")\n",
    "validation_steps = cp[\"TRAIN\"].get(\"validation_steps\")\n",
    "\n",
    "# if previously trained weights is used, never re-split\n",
    "if use_trained_model_weights:\n",
    "    # resuming mode\n",
    "    print(\"** use trained model weights **\")\n",
    "    # load training status for resuming\n",
    "    training_stats_file = os.path.join(output_dir, \".training_stats.json\")\n",
    "    if os.path.isfile(training_stats_file):\n",
    "        # TODO: add loading previous learning rate?\n",
    "        training_stats = json.load(open(training_stats_file))\n",
    "    else:\n",
    "        training_stats = {}\n",
    "else:\n",
    "    # start over\n",
    "    training_stats = {}\n",
    "\n",
    "show_model_summary = cp[\"TRAIN\"].getboolean(\"show_model_summary\")\n",
    "# end parser config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output_dir, create it if not exists\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "running_flag_file = os.path.join(output_dir, \".training.lock\")\n",
    "if os.path.isfile(running_flag_file):\n",
    "    raise RuntimeError(\"A process is running in this directory!!!\")\n",
    "else:\n",
    "    open(running_flag_file, \"a\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"backup config file to {output_dir}\")\n",
    "    shutil.copy(\n",
    "        config_file, os.path.join(output_dir, os.path.split(config_file)[1])\n",
    "    )\n",
    "\n",
    "    # loading the dataset from hub\n",
    "    url = \"s3://snark-gradient-raw-data/output_single_8_all_samples_max_4_boolean_m5_fixed_final_400/ds3/\"\n",
    "    ds = hub.Dataset(url)\n",
    "\n",
    "    # split the dataset\n",
    "    dsv_train = ds[0:140000]\n",
    "    dsv_val = ds[140000:]\n",
    "    print(\"filtering only the frontal images, this will take a few minutes\")\n",
    "    dsf_train = dsv_train.filter(only_frontal)\n",
    "    dsf_val = dsv_val.filter(only_frontal)\n",
    "    print(\"filtering completed\")\n",
    "    # get train/dev sample counts\n",
    "    train_counts, train_pos_counts = get_sample_counts(dsf_train, class_names)\n",
    "    dev_counts, _ = get_sample_counts(dsf_val, class_names)\n",
    "\n",
    "    # compute steps\n",
    "    if train_steps == \"auto\":\n",
    "        train_steps = int(train_counts / batch_size)\n",
    "    else:\n",
    "        try:\n",
    "            train_steps = int(train_steps)\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                f\"\"\"\n",
    "            train_steps: {train_steps} is invalid,\n",
    "            please use 'auto' or integer.\n",
    "            \"\"\"\n",
    "            )\n",
    "    print(f\"** train_steps: {train_steps} **\")\n",
    "\n",
    "    if validation_steps == \"auto\":\n",
    "        validation_steps = int(dev_counts / batch_size)\n",
    "    else:\n",
    "        try:\n",
    "            validation_steps = int(validation_steps)\n",
    "        except ValueError:\n",
    "            raise ValueError(\n",
    "                f\"\"\"\n",
    "            validation_steps: {validation_steps} is invalid,\n",
    "            please use 'auto' or integer.\n",
    "            \"\"\"\n",
    "            )\n",
    "    print(f\"** validation_steps: {validation_steps} **\")\n",
    "\n",
    "    print(\"** load model **\")\n",
    "    if use_trained_model_weights:\n",
    "        if use_best_weights:\n",
    "            model_weights_file = os.path.join(\n",
    "                output_dir, f\"best_{output_weights_name}\"\n",
    "            )\n",
    "        else:\n",
    "            model_weights_file = os.path.join(output_dir, output_weights_name)\n",
    "    else:\n",
    "        model_weights_file = None\n",
    "\n",
    "    model_factory = ModelFactory()\n",
    "    model = model_factory.get_model(\n",
    "        class_names,\n",
    "        model_name=base_model_name,\n",
    "        use_base_weights=use_base_model_weights,\n",
    "        weights_path=model_weights_file,\n",
    "        input_shape=(image_dimension, image_dimension, 3),\n",
    "    )\n",
    "\n",
    "    if show_model_summary:\n",
    "        print(model.summary())\n",
    "\n",
    "    # sending hub dataset to tensorflow\n",
    "    tds_train = dsf_train.to_tensorflow(\n",
    "        key_list=[\"image\", \"label_chexpert\", \"viewPosition\"])\n",
    "    tds_train = tds_train.map(to_model_fit)\n",
    "    tds_train = tds_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    tds_val = dsf_val.to_tensorflow(\n",
    "        key_list=[\"image\", \"label_chexpert\", \"viewPosition\"])\n",
    "    tds_val = tds_val.map(to_model_fit)\n",
    "    tds_val = tds_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Train data length: {len(dsf_train)}\")\n",
    "    print(f\"Val data length: {len(dsf_val)}\")\n",
    "\n",
    "    output_weights_path = os.path.join(output_dir, output_weights_name)\n",
    "    print(f\"** set output weights path to: {output_weights_path} **\")\n",
    "    optimizer = Adam(lr=initial_learning_rate)\n",
    "    print(\"** check multiple gpu availability **\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    gpus = strategy.num_replicas_in_sync\n",
    "    if gpus > 1:\n",
    "        print(f\"** multi_gpu_model is used! gpus={gpus} **\")\n",
    "        with strategy.scope():\n",
    "            model_train = model_factory.get_model(\n",
    "                class_names,\n",
    "                model_name=base_model_name,\n",
    "                use_base_weights=use_base_model_weights,\n",
    "                weights_path=model_weights_file,\n",
    "                input_shape=(image_dimension, image_dimension, 3),\n",
    "            )\n",
    "            model_train.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "    else:\n",
    "        model_train = model\n",
    "        model_train.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
    "\n",
    "    callbacks = [\n",
    "        TensorBoard(\n",
    "            log_dir=os.path.join(output_dir, \"logs\"), batch_size=batch_size\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            factor=0.1,\n",
    "            patience=patience_reduce_lr,\n",
    "            verbose=1,\n",
    "            mode=\"min\",\n",
    "            min_lr=min_lr,\n",
    "        ),\n",
    "    ]\n",
    "    print(\"** start training **\")\n",
    "    history = model_train.fit(\n",
    "        x=tds_train.repeat(),\n",
    "        steps_per_epoch=train_steps,\n",
    "        epochs=epochs,\n",
    "        validation_data=tds_val.repeat(),\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        workers=generator_workers,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # dump history\n",
    "    print(\"** dump history **\")\n",
    "    with open(os.path.join(output_dir, \"history.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"history\": history.history,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "    print(\"** done! **\")\n",
    "\n",
    "finally:\n",
    "    os.remove(running_flag_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}